{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "348b60a0-b882-4884-b048-00774824c7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (0.7.0.20230622)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: requests[socks] in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from snscrape) (2.32.3)\n",
      "Requirement already satisfied: lxml in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from snscrape) (5.3.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from snscrape) (4.13.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from snscrape) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from beautifulsoup4->snscrape) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from beautifulsoup4->snscrape) (4.13.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from requests[socks]->snscrape) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from requests[socks]->snscrape) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from requests[socks]->snscrape) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/opt/certifi/lib/python3.13/site-packages (from requests[socks]->snscrape) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install snscrape pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2956f614-6b9a-4380-9be5-c97062bdd1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Fetching page 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/lwdqxjmj54dcx0n3tktty0gw0000gn/T/ipykernel_43181/804874314.py:32: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  \"created_utc\": datetime.utcfromtimestamp(p.get(\"created_utc\", 0)).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Fetching page 2...\n",
      "ğŸ“¦ Fetching page 3...\n",
      "ğŸ“¦ Fetching page 4...\n",
      "ğŸ“¦ Fetching page 5...\n",
      "ğŸ“¦ Fetching page 6...\n",
      "ğŸ“¦ Fetching page 7...\n",
      "ğŸ“¦ Fetching page 8...\n",
      "ğŸ“¦ Fetching page 9...\n",
      "ğŸ“¦ Fetching page 10...\n",
      "âœ… Collected 252 posts total\n",
      "                                               title                author  \\\n",
      "0  Trade Without Trade-Offs on Kraken Pro! + (Moo...        krakenexchange   \n",
      "1    Daily Crypto Discussion - April 8, 2025 (GMT+0)          CryptoDaily-   \n",
      "2  U.S government suffers 26% loss on crypto sinc...               Abdeliq   \n",
      "3  Just over two month ago Eric Trump was shillin...              GabeSter   \n",
      "4  Dave Ramsey Predicted Bitcoin Would Crash to $...  InclineDumbbellPress   \n",
      "\n",
      "           created_utc  score  num_comments  \\\n",
      "0  2025-04-04 14:05:29     22            66   \n",
      "1  2025-04-08 00:00:47      8           134   \n",
      "2  2025-04-07 10:57:17   2293           304   \n",
      "3  2025-04-07 13:24:11   1011            89   \n",
      "4  2025-04-08 00:38:36    110            35   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.reddit.com/r/CryptoCurrency/commen...  \n",
      "1  https://www.reddit.com/r/CryptoCurrency/commen...  \n",
      "2  https://finbold.com/u-s-government-suffers-26-...  \n",
      "3                https://i.redd.it/2bs6c6lzyete1.png  \n",
      "4                https://i.redd.it/mnlu1q7laite1.png  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def scrape_multiple_pages(subreddit=\"CryptoCurrency\", pages=10, delay=2):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    after = None\n",
    "    all_posts = []\n",
    "\n",
    "    for page in range(pages):\n",
    "        url = f\"https://www.reddit.com/r/{subreddit}/.json\"\n",
    "        if after:\n",
    "            url += f\"?after={after}\"\n",
    "\n",
    "        print(f\"ğŸ“¦ Fetching page {page + 1}...\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(\"âŒ Failed at page\", page + 1, \"| Status:\", response.status_code)\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        posts = data[\"data\"][\"children\"]\n",
    "        after = data[\"data\"].get(\"after\", None)\n",
    "\n",
    "        for post in posts:\n",
    "            p = post[\"data\"]\n",
    "            all_posts.append({\n",
    "                \"title\": p.get(\"title\", \"\"),\n",
    "                \"author\": p.get(\"author\", \"\"),\n",
    "                \"created_utc\": datetime.utcfromtimestamp(p.get(\"created_utc\", 0)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"score\": p.get(\"score\", 0),\n",
    "                \"num_comments\": p.get(\"num_comments\", 0),\n",
    "                \"url\": p.get(\"url\", \"\")\n",
    "            })\n",
    "\n",
    "        if not after:\n",
    "            print(\"ğŸš« No more pages.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(delay)  # Be polite to Reddit\n",
    "\n",
    "    df = pd.DataFrame(all_posts)\n",
    "    return df\n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_multiple_pages(pages=10)\n",
    "    df.to_csv(\"r_cryptocurrency_multi.csv\", index=False)\n",
    "    print(f\"âœ… Collected {len(df)} posts total\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be45a30f-76fd-4624-8c21-ca5aa3aee2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“œ Scrolling... 1\n",
      "ğŸ“œ Scrolling... 2\n",
      "ğŸš« No more content to scroll.\n",
      "âœ… Saved 5 tweets\n",
      "           username    date  \\\n",
      "0    @Cointelegraph      1h   \n",
      "1          @GENIC0N   Apr 3   \n",
      "2     @JhonBismarck      6h   \n",
      "3           @farokh  Jan 10   \n",
      "4  @AirdropNinjaPro   Apr 5   \n",
      "\n",
      "                                                text  \\\n",
      "0  âš¡NEW: A trader got liquidated for $100M yester...   \n",
      "1  Anime cat girls will be mass manufactured in u...   \n",
      "2  AVM will revolutionize the Bitcoin ecosystem, ...   \n",
      "3  I attended the opening of the new @Ledger HQ i...   \n",
      "4  New Airdrop: SALTY\\nReward: 150,000 SALTY \\nLi...   \n",
      "\n",
      "                                                 url  \n",
      "0  https://nitter.net/Cointelegraph/status/190944...  \n",
      "1  https://nitter.net/GENIC0N/status/190762753788...  \n",
      "2  https://nitter.net/JhonBismarck/status/1909378...  \n",
      "3  https://nitter.net/farokh/status/1877716958025...  \n",
      "4  https://nitter.net/AirdropNinjaPro/status/1908...  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Debug visible\n",
    "chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 13_0)\")\n",
    "\n",
    "webdriver_service = Service(\"/opt/homebrew/bin/chromedriver\")\n",
    "driver = webdriver.Chrome(service=webdriver_service, options=chrome_options)\n",
    "\n",
    "def scrape_nitter(query=\"crypto OR bitcoin OR ethereum\", max_scrolls=10, delay=3):\n",
    "    base_url = \"https://nitter.net/search\"\n",
    "    driver.get(f\"{base_url}?f=tweets&q={query}\")\n",
    "    tweets_data = set()\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for scroll in range(max_scrolls):\n",
    "        print(f\"ğŸ“œ Scrolling... {scroll + 1}\")\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            print(\"ğŸš« No more content to scroll.\")\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "        tweets = driver.find_elements(By.CLASS_NAME, \"timeline-item\")\n",
    "\n",
    "        for tweet in tweets:\n",
    "            try:\n",
    "                username = tweet.find_element(By.CLASS_NAME, \"username\").text\n",
    "                date = tweet.find_element(By.CSS_SELECTOR, \"span.tweet-date a\").text\n",
    "                tweet_url = tweet.find_element(By.CSS_SELECTOR, \"span.tweet-date a\").get_attribute(\"href\")\n",
    "                text = tweet.find_element(By.CLASS_NAME, \"tweet-content\").text\n",
    "\n",
    "                tweet_id = f\"{username}|{date}|{text[:20]}\"  # Unique-ish key\n",
    "                if tweet_id not in tweets_data:\n",
    "                    tweets_data.add(tweet_id)\n",
    "                    yield {\n",
    "                        \"username\": username,\n",
    "                        \"date\": date,\n",
    "                        \"text\": text,\n",
    "                        \"url\": tweet_url if \"nitter.net\" in tweet_url else f\"https://nitter.net{tweet_url}\"\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                print(\"âš ï¸ Skipping tweet:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collected = list(scrape_nitter(max_scrolls=20))\n",
    "    df = pd.DataFrame(collected)\n",
    "    df.to_csv(\"nitter_crypto_broad.csv\", index=False)\n",
    "    print(f\"âœ… Saved {len(df)} tweets\")\n",
    "    print(df.head())\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37a85dfa-2331-4f08-902f-fad1cdb28ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scraped 52 headlines from CoinDesk.\n",
      "             headline                                          url  \\\n",
      "0  View price details       https://www.coindesk.com/price/bitcoin   \n",
      "1  View price details      https://www.coindesk.com/price/ethereum   \n",
      "2  View price details        https://www.coindesk.com/price/tether   \n",
      "3  View price details           https://www.coindesk.com/price/xrp   \n",
      "4  View price details  https://www.coindesk.com/price/binance-coin   \n",
      "\n",
      "            scraped_at  \n",
      "0  2025-04-08 00:08:49  \n",
      "1  2025-04-08 00:08:49  \n",
      "2  2025-04-08 00:08:49  \n",
      "3  2025-04-08 00:08:49  \n",
      "4  2025-04-08 00:08:49  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# ğŸ”§ Set up headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Remove if you want to see browser\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "\n",
    "# âœ… Update this to your ChromeDriver path\n",
    "driver_path = \"/opt/homebrew/bin/chromedriver\"  # Change if needed\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def scrape_coindesk_headlines():\n",
    "    driver.get(\"https://www.coindesk.com/\")\n",
    "    time.sleep(5)  # Give time for JS to load\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    headlines = []\n",
    "\n",
    "    # Find all anchor tags with title attribute and class indicating headline\n",
    "    for a in soup.find_all(\"a\", attrs={\"title\": True, \"href\": True}):\n",
    "        title = a[\"title\"].strip()\n",
    "        href = a[\"href\"].strip()\n",
    "        if href.startswith(\"/\"):  # relative path\n",
    "            full_url = \"https://www.coindesk.com\" + href\n",
    "        else:\n",
    "            full_url = href\n",
    "\n",
    "        headlines.append({\n",
    "            \"headline\": title,\n",
    "            \"url\": full_url,\n",
    "            \"scraped_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(headlines)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_coindesk_headlines()\n",
    "    driver.quit()\n",
    "\n",
    "    df.drop_duplicates(subset=\"url\", inplace=True)\n",
    "    df.to_csv(\"coindesk_latest_news.csv\", index=False)\n",
    "    print(f\"âœ… Scraped {len(df)} headlines from CoinDesk.\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "642579ae-a6f9-4653-b390-1e3408e8c175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Fetching r/CryptoCurrency page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/lwdqxjmj54dcx0n3tktty0gw0000gn/T/ipykernel_43181/2327192113.py:32: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  \"created_utc\": datetime.utcfromtimestamp(p.get(\"created_utc\", 0)).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Fetching r/CryptoCurrency page 2\n",
      "ğŸ“¦ Fetching r/CryptoCurrency page 3\n",
      "ğŸ“¦ Fetching r/CryptoCurrency page 4\n",
      "ğŸ“¦ Fetching r/CryptoCurrency page 5\n",
      "ğŸ“¦ Fetching r/CryptoCurrency page 6\n",
      "ğŸ“¦ Fetching r/CryptoCurrency page 7\n",
      "ğŸ“¦ Fetching r/CryptoCurrency page 8\n",
      "ğŸ“¦ Fetching r/CryptoCurrency page 9\n",
      "ğŸ“¦ Fetching r/CryptoCurrency page 10\n",
      "ğŸ“¦ Fetching r/CryptoCurrency page 11\n",
      "ğŸ“¦ Fetching r/CryptoCurrency page 12\n",
      "ğŸ“¦ Fetching r/CryptoCurrency page 13\n",
      "ğŸ“¦ Fetching r/CryptoCurrency page 14\n",
      "ğŸ“¦ Fetching r/CryptoCurrency page 15\n",
      "ğŸš« No more pages for r/CryptoCurrency\n",
      "ğŸ“¦ Fetching r/Bitcoin page 1\n",
      "ğŸ“¦ Fetching r/Bitcoin page 2\n",
      "ğŸ“¦ Fetching r/Bitcoin page 3\n",
      "ğŸ“¦ Fetching r/Bitcoin page 4\n",
      "ğŸ“¦ Fetching r/Bitcoin page 5\n",
      "ğŸ“¦ Fetching r/Bitcoin page 6\n",
      "ğŸ“¦ Fetching r/Bitcoin page 7\n",
      "ğŸ“¦ Fetching r/Bitcoin page 8\n",
      "ğŸ“¦ Fetching r/Bitcoin page 9\n",
      "ğŸ“¦ Fetching r/Bitcoin page 10\n",
      "ğŸ“¦ Fetching r/Bitcoin page 11\n",
      "ğŸ“¦ Fetching r/Bitcoin page 12\n",
      "ğŸ“¦ Fetching r/Bitcoin page 13\n",
      "ğŸ“¦ Fetching r/Bitcoin page 14\n",
      "ğŸ“¦ Fetching r/Bitcoin page 15\n",
      "ğŸ“¦ Fetching r/Bitcoin page 16\n",
      "ğŸ“¦ Fetching r/Bitcoin page 17\n",
      "ğŸ“¦ Fetching r/Bitcoin page 18\n",
      "ğŸ“¦ Fetching r/Bitcoin page 19\n",
      "ğŸ“¦ Fetching r/Bitcoin page 20\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 1\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 2\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 3\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 4\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 5\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 6\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 7\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 8\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 9\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 10\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 11\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 12\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 13\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 14\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 15\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 16\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 17\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 18\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 19\n",
      "ğŸ“¦ Fetching r/CryptoMarkets page 20\n",
      "ğŸ“¦ Fetching r/CryptoTechnology page 1\n",
      "ğŸ“¦ Fetching r/CryptoTechnology page 2\n",
      "ğŸ“¦ Fetching r/CryptoTechnology page 3\n",
      "ğŸ“¦ Fetching r/CryptoTechnology page 4\n",
      "ğŸ“¦ Fetching r/CryptoTechnology page 5\n",
      "ğŸ“¦ Fetching r/CryptoTechnology page 6\n",
      "ğŸ“¦ Fetching r/CryptoTechnology page 7\n",
      "ğŸš« No more pages for r/CryptoTechnology\n",
      "ğŸ“¦ Fetching r/CryptoNews page 1\n",
      "ğŸ“¦ Fetching r/CryptoNews page 2\n",
      "ğŸ“¦ Fetching r/CryptoNews page 3\n",
      "ğŸ“¦ Fetching r/CryptoNews page 4\n",
      "ğŸ“¦ Fetching r/CryptoNews page 5\n",
      "ğŸ“¦ Fetching r/CryptoNews page 6\n",
      "ğŸ“¦ Fetching r/CryptoNews page 7\n",
      "ğŸ“¦ Fetching r/CryptoNews page 8\n",
      "ğŸ“¦ Fetching r/CryptoNews page 9\n",
      "ğŸ“¦ Fetching r/CryptoNews page 10\n",
      "ğŸ“¦ Fetching r/CryptoNews page 11\n",
      "ğŸ“¦ Fetching r/CryptoNews page 12\n",
      "ğŸ“¦ Fetching r/CryptoNews page 13\n",
      "ğŸ“¦ Fetching r/CryptoNews page 14\n",
      "ğŸ“¦ Fetching r/CryptoNews page 15\n",
      "ğŸ“¦ Fetching r/CryptoNews page 16\n",
      "ğŸ“¦ Fetching r/CryptoNews page 17\n",
      "ğŸ“¦ Fetching r/CryptoNews page 18\n",
      "ğŸ“¦ Fetching r/CryptoNews page 19\n",
      "ğŸ“¦ Fetching r/CryptoNews page 20\n",
      "âœ… Collected 2021 total posts across 5 subreddits.\n",
      "        subreddit                                              title  \\\n",
      "0  CryptoCurrency  Trade Without Trade-Offs on Kraken Pro! + (Moo...   \n",
      "1  CryptoCurrency    Daily Crypto Discussion - April 8, 2025 (GMT+0)   \n",
      "2  CryptoCurrency  U.S government suffers 26% loss on crypto sinc...   \n",
      "3  CryptoCurrency  Just over two month ago Eric Trump was shillin...   \n",
      "4  CryptoCurrency  Dave Ramsey Predicted Bitcoin Would Crash to $...   \n",
      "\n",
      "                 author          created_utc  score  num_comments  \\\n",
      "0        krakenexchange  2025-04-04 14:05:29     23            66   \n",
      "1          CryptoDaily-  2025-04-08 00:00:47      9           143   \n",
      "2               Abdeliq  2025-04-07 10:57:17   2322           305   \n",
      "3              GabeSter  2025-04-07 13:24:11   1034            94   \n",
      "4  InclineDumbbellPress  2025-04-08 00:38:36    122            37   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.reddit.com/r/CryptoCurrency/commen...  \n",
      "1  https://www.reddit.com/r/CryptoCurrency/commen...  \n",
      "2  https://finbold.com/u-s-government-suffers-26-...  \n",
      "3                https://i.redd.it/2bs6c6lzyete1.png  \n",
      "4                https://i.redd.it/mnlu1q7laite1.png  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def scrape_reddit(subreddit=\"CryptoCurrency\", pages=20, delay=2):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    after = None\n",
    "    posts = []\n",
    "\n",
    "    for page in range(pages):\n",
    "        url = f\"https://www.reddit.com/r/{subreddit}/.json\"\n",
    "        if after:\n",
    "            url += f\"?after={after}\"\n",
    "\n",
    "        print(f\"ğŸ“¦ Fetching r/{subreddit} page {page + 1}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"âŒ Failed at page {page + 1} | Status:\", response.status_code)\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        children = data[\"data\"][\"children\"]\n",
    "        after = data[\"data\"].get(\"after\", None)\n",
    "\n",
    "        for post in children:\n",
    "            p = post[\"data\"]\n",
    "            posts.append({\n",
    "                \"subreddit\": subreddit,\n",
    "                \"title\": p.get(\"title\", \"\"),\n",
    "                \"author\": p.get(\"author\", \"\"),\n",
    "                \"created_utc\": datetime.utcfromtimestamp(p.get(\"created_utc\", 0)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"score\": p.get(\"score\", 0),\n",
    "                \"num_comments\": p.get(\"num_comments\", 0),\n",
    "                \"url\": p.get(\"url\", \"\")\n",
    "            })\n",
    "\n",
    "        if not after:\n",
    "            print(f\"ğŸš« No more pages for r/{subreddit}\")\n",
    "            break\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return posts\n",
    "\n",
    "# ğŸ” Combine from multiple subreddits\n",
    "if __name__ == \"__main__\":\n",
    "    all_subreddits = [\"CryptoCurrency\", \"Bitcoin\", \"CryptoMarkets\", \"CryptoTechnology\", \"CryptoNews\"]\n",
    "    all_posts = []\n",
    "\n",
    "    for sub in all_subreddits:\n",
    "        posts = scrape_reddit(subreddit=sub, pages=20)\n",
    "        all_posts.extend(posts)\n",
    "\n",
    "    df = pd.DataFrame(all_posts)\n",
    "    df.drop_duplicates(subset=[\"title\", \"url\"], inplace=True)\n",
    "    df.to_csv(\"reddit_crypto_bulk.csv\", index=False)\n",
    "\n",
    "    print(f\"âœ… Collected {len(df)} total posts across {len(all_subreddits)} subreddits.\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3643ee06-7b9f-4816-aa0d-5cb8aacc318f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Starting r/CryptoCurrency...\n",
      "ğŸ“¦ Fetching page 1 of r/CryptoCurrency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/lwdqxjmj54dcx0n3tktty0gw0000gn/T/ipykernel_43181/2401029582.py:34: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  \"created_utc\": datetime.utcfromtimestamp(p.get(\"created_utc\", 0)).strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Fetching page 2 of r/CryptoCurrency\n",
      "ğŸ“¦ Fetching page 3 of r/CryptoCurrency\n",
      "ğŸ“¦ Fetching page 4 of r/CryptoCurrency\n",
      "ğŸ“¦ Fetching page 5 of r/CryptoCurrency\n",
      "ğŸ“¦ Fetching page 6 of r/CryptoCurrency\n",
      "ğŸ“¦ Fetching page 7 of r/CryptoCurrency\n",
      "ğŸ“¦ Fetching page 8 of r/CryptoCurrency\n",
      "ğŸ“¦ Fetching page 9 of r/CryptoCurrency\n",
      "ğŸ“¦ Fetching page 10 of r/CryptoCurrency\n",
      "ğŸ“¦ Fetching page 11 of r/CryptoCurrency\n",
      "ğŸ“¦ Fetching page 12 of r/CryptoCurrency\n",
      "ğŸ“¦ Fetching page 13 of r/CryptoCurrency\n",
      "ğŸ“¦ Fetching page 14 of r/CryptoCurrency\n",
      "ğŸ“¦ Fetching page 15 of r/CryptoCurrency\n",
      "ğŸš« No more pages.\n",
      "\n",
      "ğŸš€ Starting r/Bitcoin...\n",
      "ğŸ“¦ Fetching page 1 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 2 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 3 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 4 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 5 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 6 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 7 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 8 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 9 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 10 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 11 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 12 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 13 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 14 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 15 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 16 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 17 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 18 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 19 of r/Bitcoin\n",
      "ğŸ“¦ Fetching page 20 of r/Bitcoin\n",
      "\n",
      "ğŸš€ Starting r/CryptoMarkets...\n",
      "ğŸ“¦ Fetching page 1 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 2 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 3 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 4 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 5 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 6 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 7 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 8 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 9 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 10 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 11 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 12 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 13 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 14 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 15 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 16 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 17 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 18 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 19 of r/CryptoMarkets\n",
      "ğŸ“¦ Fetching page 20 of r/CryptoMarkets\n",
      "\n",
      "ğŸš€ Starting r/CryptoTechnology...\n",
      "ğŸ“¦ Fetching page 1 of r/CryptoTechnology\n",
      "ğŸ“¦ Fetching page 2 of r/CryptoTechnology\n",
      "ğŸ“¦ Fetching page 3 of r/CryptoTechnology\n",
      "ğŸ“¦ Fetching page 4 of r/CryptoTechnology\n",
      "ğŸ“¦ Fetching page 5 of r/CryptoTechnology\n",
      "ğŸ“¦ Fetching page 6 of r/CryptoTechnology\n",
      "ğŸ“¦ Fetching page 7 of r/CryptoTechnology\n",
      "ğŸš« No more pages.\n",
      "\n",
      "ğŸš€ Starting r/CryptoNews...\n",
      "ğŸ“¦ Fetching page 1 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 2 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 3 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 4 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 5 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 6 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 7 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 8 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 9 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 10 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 11 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 12 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 13 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 14 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 15 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 16 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 17 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 18 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 19 of r/CryptoNews\n",
      "ğŸ“¦ Fetching page 20 of r/CryptoNews\n",
      "\n",
      "âœ… DONE: Collected 2021 Reddit posts total across 5 subs.\n",
      "       subreddit                                              title  \\\n",
      "605      Bitcoin  To the guy talking about converting half his 4...   \n",
      "816      Bitcoin                                    105 connections   \n",
      "1670  CryptoNews  Lawmakers Urge HUD to Suspend Plans for Crypto...   \n",
      "422      Bitcoin                             I see signs everywhere   \n",
      "1858  CryptoNews  World Network Falls Short: 988M Users Away fro...   \n",
      "\n",
      "               author          created_utc  score  num_comments  \\\n",
      "605       Bitcoin401k  2025-04-03 21:40:15    141            33   \n",
      "816   DreamingTooLong  2025-03-31 15:01:16     26             6   \n",
      "1670       bitnewsbot  2025-04-02 16:15:31      1             0   \n",
      "422      BernardMarxx  2025-04-07 00:58:22     67             4   \n",
      "1858       bitnewsbot  2025-03-24 21:44:44      1             0   \n",
      "\n",
      "                                                    url  \n",
      "605   https://www.reddit.com/r/Bitcoin/comments/1jqv...  \n",
      "816                https://i.redd.it/y5zz0ve8i1se1.jpeg  \n",
      "1670  https://bitnewsbot.com/lawmakers-urge-hud-to-s...  \n",
      "422                https://i.redd.it/zb5siuc8abte1.jpeg  \n",
      "1858  https://bitnewsbot.com/world-network-falls-sho...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def scrape_reddit_bulk(subreddits, pages_per_sub=20, delay=2):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    all_posts = []\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        after = None\n",
    "        print(f\"\\nğŸš€ Starting r/{subreddit}...\")\n",
    "        for page in range(pages_per_sub):\n",
    "            url = f\"https://www.reddit.com/r/{subreddit}/.json\"\n",
    "            if after:\n",
    "                url += f\"?after={after}\"\n",
    "\n",
    "            print(f\"ğŸ“¦ Fetching page {page + 1} of r/{subreddit}\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"âŒ Failed at page {page + 1} | Status Code:\", response.status_code)\n",
    "                break\n",
    "\n",
    "            data = response.json()\n",
    "            posts = data[\"data\"][\"children\"]\n",
    "            after = data[\"data\"].get(\"after\", None)\n",
    "\n",
    "            for post in posts:\n",
    "                p = post[\"data\"]\n",
    "                all_posts.append({\n",
    "                    \"subreddit\": subreddit,\n",
    "                    \"title\": p.get(\"title\", \"\"),\n",
    "                    \"author\": p.get(\"author\", \"\"),\n",
    "                    \"created_utc\": datetime.utcfromtimestamp(p.get(\"created_utc\", 0)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    \"score\": p.get(\"score\", 0),\n",
    "                    \"num_comments\": p.get(\"num_comments\", 0),\n",
    "                    \"url\": p.get(\"url\", \"\")\n",
    "                })\n",
    "\n",
    "            if not after:\n",
    "                print(\"ğŸš« No more pages.\")\n",
    "                break\n",
    "            time.sleep(delay)\n",
    "\n",
    "    return pd.DataFrame(all_posts)\n",
    "\n",
    "# ğŸ” Run the collector\n",
    "if __name__ == \"__main__\":\n",
    "    target_subreddits = [\n",
    "        \"CryptoCurrency\",\n",
    "        \"Bitcoin\",\n",
    "        \"CryptoMarkets\",\n",
    "        \"CryptoTechnology\",\n",
    "        \"CryptoNews\"\n",
    "    ]\n",
    "\n",
    "    df = scrape_reddit_bulk(target_subreddits, pages_per_sub=20)  # 20 pages x ~25 posts x 5 subs â‰ˆ 2500+\n",
    "    df.drop_duplicates(subset=[\"title\", \"url\"], inplace=True)\n",
    "    df.to_csv(\"reddit_crypto_2000_posts.csv\", index=False)\n",
    "\n",
    "    print(f\"\\nâœ… DONE: Collected {len(df)} Reddit posts total across {len(target_subreddits)} subs.\")\n",
    "    print(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdb6db21-250c-494f-a9d6-962fbcfed027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Opening: https://cointelegraph.com/tags/bitcoin\n",
      "ğŸ“œ Scroll 1/150\n",
      "ğŸ“œ Scroll 2/150\n",
      "ğŸ“œ Scroll 3/150\n",
      "ğŸ“œ Scroll 4/150\n",
      "ğŸ“œ Scroll 5/150\n",
      "ğŸ“œ Scroll 6/150\n",
      "ğŸ“œ Scroll 7/150\n",
      "ğŸš« Reached end of page.\n",
      "\n",
      "âœ… Scraped 105 Bitcoin articles from CoinTelegraph\n",
      "           source      tag                                           headline  \\\n",
      "79  CoinTelegraph  bitcoin  GameStop finishes $1.5B raise to add Bitcoin t...   \n",
      "46  CoinTelegraph  bitcoin  Trump tariffs squeeze already struggling Bitco...   \n",
      "86  CoinTelegraph  bitcoin  Bitcoin mining using coal energy down 43% sinc...   \n",
      "29  CoinTelegraph  bitcoin  No country wins a global trade war, BTC to sur...   \n",
      "4   CoinTelegraph  bitcoin               Hereâ€™s what happened in crypto today   \n",
      "\n",
      "                                                  url           scraped_at  \n",
      "79  https://cointelegraph.com/news/game-stop-finis...  2025-04-08 06:48:16  \n",
      "46  https://cointelegraph.com/news/trump-tariffs-b...  2025-04-08 06:48:16  \n",
      "86  https://cointelegraph.com/news/bitcoin-mining-...  2025-04-08 06:48:16  \n",
      "29  https://cointelegraph.com/news/no-country-wins...  2025-04-08 06:48:16  \n",
      "4   https://cointelegraph.com/news/what-happened-i...  2025-04-08 06:48:16  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/lwdqxjmj54dcx0n3tktty0gw0000gn/T/ipykernel_43181/1473940868.py:53: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"scraped_at\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def scroll_to_bottom(driver, scrolls=100, delay=2):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for i in range(scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(delay)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        print(f\"ğŸ“œ Scroll {i+1}/{scrolls}\")\n",
    "        if new_height == last_height:\n",
    "            print(\"ğŸš« Reached end of page.\")\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "def scrape_cointelegraph_bitcoin_articles():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "\n",
    "    driver_path = \"/opt/homebrew/bin/chromedriver\"  # Update path if needed\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    url = \"https://cointelegraph.com/tags/bitcoin\"\n",
    "    print(f\"ğŸŒ Opening: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    scroll_to_bottom(driver, scrolls=150, delay=2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    links = soup.select(\"a.post-card-inline__title-link\")\n",
    "\n",
    "    articles = []\n",
    "    for a in links:\n",
    "        headline = a.text.strip()\n",
    "        href = a.get(\"href\", \"\")\n",
    "        if headline and href:\n",
    "            full_url = \"https://cointelegraph.com\" + href if href.startswith(\"/\") else href\n",
    "            articles.append({\n",
    "                \"source\": \"CoinTelegraph\",\n",
    "                \"tag\": \"bitcoin\",\n",
    "                \"headline\": headline,\n",
    "                \"url\": full_url,\n",
    "                \"scraped_at\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_cointelegraph_bitcoin_articles()\n",
    "    df.drop_duplicates(subset=\"url\", inplace=True)\n",
    "    df.to_excel(\"cointelegraph_bitcoin_scroll_fixed.xlsx\", index=False)\n",
    "\n",
    "    print(f\"\\nâœ… Scraped {len(df)} Bitcoin articles from CoinTelegraph\")\n",
    "    print(df.sample(min(5, len(df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13243115-28dd-4a1d-8bdf-329d287bc951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Fetching page 1: https://cryptoslate.com/wp-json/wp/v2/posts?page=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/lwdqxjmj54dcx0n3tktty0gw0000gn/T/ipykernel_43181/3652343853.py:33: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"scraped_at\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Fetching page 2: https://cryptoslate.com/wp-json/wp/v2/posts?page=2\n",
      "ğŸ”„ Fetching page 3: https://cryptoslate.com/wp-json/wp/v2/posts?page=3\n",
      "ğŸ”„ Fetching page 4: https://cryptoslate.com/wp-json/wp/v2/posts?page=4\n",
      "ğŸ”„ Fetching page 5: https://cryptoslate.com/wp-json/wp/v2/posts?page=5\n",
      "ğŸ”„ Fetching page 6: https://cryptoslate.com/wp-json/wp/v2/posts?page=6\n",
      "ğŸ”„ Fetching page 7: https://cryptoslate.com/wp-json/wp/v2/posts?page=7\n",
      "ğŸ”„ Fetching page 8: https://cryptoslate.com/wp-json/wp/v2/posts?page=8\n",
      "ğŸ”„ Fetching page 9: https://cryptoslate.com/wp-json/wp/v2/posts?page=9\n",
      "ğŸ”„ Fetching page 10: https://cryptoslate.com/wp-json/wp/v2/posts?page=10\n",
      "ğŸ”„ Fetching page 11: https://cryptoslate.com/wp-json/wp/v2/posts?page=11\n",
      "ğŸ”„ Fetching page 12: https://cryptoslate.com/wp-json/wp/v2/posts?page=12\n",
      "ğŸ”„ Fetching page 13: https://cryptoslate.com/wp-json/wp/v2/posts?page=13\n",
      "ğŸ”„ Fetching page 14: https://cryptoslate.com/wp-json/wp/v2/posts?page=14\n",
      "ğŸ”„ Fetching page 15: https://cryptoslate.com/wp-json/wp/v2/posts?page=15\n",
      "ğŸ”„ Fetching page 16: https://cryptoslate.com/wp-json/wp/v2/posts?page=16\n",
      "ğŸ”„ Fetching page 17: https://cryptoslate.com/wp-json/wp/v2/posts?page=17\n",
      "ğŸ”„ Fetching page 18: https://cryptoslate.com/wp-json/wp/v2/posts?page=18\n",
      "ğŸ”„ Fetching page 19: https://cryptoslate.com/wp-json/wp/v2/posts?page=19\n",
      "ğŸ”„ Fetching page 20: https://cryptoslate.com/wp-json/wp/v2/posts?page=20\n",
      "ğŸ”„ Fetching page 21: https://cryptoslate.com/wp-json/wp/v2/posts?page=21\n",
      "ğŸ”„ Fetching page 22: https://cryptoslate.com/wp-json/wp/v2/posts?page=22\n",
      "ğŸ”„ Fetching page 23: https://cryptoslate.com/wp-json/wp/v2/posts?page=23\n",
      "ğŸ”„ Fetching page 24: https://cryptoslate.com/wp-json/wp/v2/posts?page=24\n",
      "ğŸ”„ Fetching page 25: https://cryptoslate.com/wp-json/wp/v2/posts?page=25\n",
      "âœ… Saved 250 articles to cryptoslate_articles_wpapi.xlsx\n",
      "                                                 title  \\\n",
      "219  Pumpfun launches its own DEX called PumpSwap a...   \n",
      "99   BlackRock&#8217;s Larry Fink confirms Bitcoin ...   \n",
      "188  Verifiable AI: The key to balancing innovation...   \n",
      "140  Congressman Emmer reintroduces Securities Clar...   \n",
      "128  GameStop stock slides 25% as investor skeptici...   \n",
      "\n",
      "                                                  link                 date  \\\n",
      "219  https://cryptoslate.com/pumpfun-launches-its-o...  2025-03-20T19:15:54   \n",
      "99   https://cryptoslate.com/blackrocks-larry-fink-...  2025-03-31T15:00:30   \n",
      "188  https://cryptoslate.com/verifiable-ai-the-key-...  2025-03-24T04:00:41   \n",
      "140  https://cryptoslate.com/congressman-emmer-rein...  2025-03-27T01:00:30   \n",
      "128  https://cryptoslate.com/gamestop-stock-slides-...  2025-03-27T22:30:07   \n",
      "\n",
      "              scraped_at  \n",
      "219  2025-04-08 07:02:10  \n",
      "99   2025-04-08 07:01:52  \n",
      "188  2025-04-08 07:02:06  \n",
      "140  2025-04-08 07:02:00  \n",
      "128  2025-04-08 07:01:57  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def scrape_cryptoslate_wpapi(pages=10):\n",
    "    base_url = \"https://cryptoslate.com/wp-json/wp/v2/posts?page={}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    all_articles = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        url = base_url.format(page)\n",
    "        print(f\"ğŸ”„ Fetching page {page}: {url}\")\n",
    "        resp = requests.get(url, headers=headers)\n",
    "\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"âŒ Failed at page {page} | Status {resp.status_code}\")\n",
    "            break\n",
    "\n",
    "        data = resp.json()\n",
    "        if not data:\n",
    "            print(\"ğŸš« No data returned.\")\n",
    "            break\n",
    "\n",
    "        for item in data:\n",
    "            all_articles.append({\n",
    "                \"title\": item.get(\"title\", {}).get(\"rendered\", \"\"),\n",
    "                \"link\": item.get(\"link\", \"\"),\n",
    "                \"date\": item.get(\"date\", \"\"),\n",
    "                \"scraped_at\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    return pd.DataFrame(all_articles)\n",
    "\n",
    "# âœ… Run it\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_cryptoslate_wpapi(pages=25)\n",
    "    df.to_excel(\"cryptoslate_articles_wpapi.xlsx\", index=False)\n",
    "    print(f\"âœ… Saved {len(df)} articles to cryptoslate_articles_wpapi.xlsx\")\n",
    "    print(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "19bf9598-b933-482c-881a-4c9261b1b99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (0.28.1)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from httpx) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/opt/certifi/lib/python3.13/site-packages (from httpx) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from httpx) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from httpx) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from httpcore==1.*->httpx) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from anyio->httpx) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4065fbad-3c2d-4374-8a5b-2534a7ffb232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx[http2] in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (0.28.1)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from httpx[http2]) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/opt/certifi/lib/python3.13/site-packages (from httpx[http2]) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from httpx[http2]) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from httpx[http2]) (3.10)\n",
      "Collecting h2<5,>=3 (from httpx[http2])\n",
      "  Downloading h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from httpcore==1.*->httpx[http2]) (0.14.0)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2])\n",
      "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2])\n",
      "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages (from anyio->httpx[http2]) (1.3.1)\n",
      "Downloading h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: hyperframe, hpack, h2\n",
      "Successfully installed h2-4.2.0 hpack-4.1.0 hyperframe-6.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"httpx[http2]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4888c99b-a71d-4192-affc-c0daf8074bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Opening CoinSpeaker News Page...\n",
      "ğŸ“œ Scroll 1/4\n",
      "ğŸ“œ Scroll 2/4\n",
      "ğŸ“œ Scroll 3/4\n",
      "ğŸ“œ Scroll 4/4\n",
      "âŒ Timed out or no articles found.\n",
      "\n",
      "âœ… Scraped 0 articles from CoinSpeaker (4 scrolls).\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Setup Chrome driver\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "\n",
    "driver_path = \"/opt/homebrew/bin/chromedriver\"  # Change if necessary\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "def scrape_coinspeaker_news(short_scrolls=4, delay=3):\n",
    "    print(\"ğŸŒ Opening CoinSpeaker News Page...\")\n",
    "    driver.get(\"https://www.coinspeaker.com/news/\")\n",
    "    articles = []\n",
    "\n",
    "    for i in range(short_scrolls):\n",
    "        print(f\"ğŸ“œ Scroll {i + 1}/{short_scrolls}\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "    # Wait for at least 1 article to show up\n",
    "    try:\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"news-flow-card\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"âŒ Timed out or no articles found.\")\n",
    "        driver.quit()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    cards = soup.find_all(\"div\", class_=\"news-flow-card\")\n",
    "\n",
    "    for card in cards:\n",
    "        a_tag = card.find(\"a\", class_=\"title\")\n",
    "        if a_tag:\n",
    "            title = a_tag.get_text(strip=True)\n",
    "            href = a_tag.get(\"href\")\n",
    "            url = href if href.startswith(\"http\") else f\"https://www.coinspeaker.com{href}\"\n",
    "            articles.append({\"title\": title, \"url\": url})\n",
    "\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "# Run it for testing 4 scrolls\n",
    "df = scrape_coinspeaker_news(short_scrolls=4)\n",
    "df.drop_duplicates(subset=\"url\", inplace=True)\n",
    "df.to_csv(\"coinspeaker_short_scroll_news.csv\", index=False)\n",
    "df.to_excel(\"coinspeaker_short_scroll_news.xlsx\", index=False)\n",
    "\n",
    "print(f\"\\nâœ… Scraped {len(df)} articles from CoinSpeaker (4 scrolls).\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe2fdd-942d-4506-ac27-2a18654c6e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
